---
title: "Homework 5"
subtitle: <center> <h1>Multiple Linear Regression Variable Selection Methods</h1> </center>
author: <center> Jake Hiltscher <center>
output: html_document
---

<style type="text/css">
h1.title {
font-size: 40px;
text-align: center;
}
</style>

```{r setup, include=FALSE}
# load packages here
library(tidyverse)
library(corrplot)  # colored correlation matrix
library(ggfortify)  # plot glmnet objects using ggplot instead of base R
library(car)  # needed for VIFs
library(bestglm)  # for stepwise methods
library(glmnet)  # for ridge, lasso, and elastic net
library(gridExtra)
set.seed(12345)  # make sure to set your seed when doing cross validation!
```

## Data and Description

**For this assignment, we are revisiting the data set used in Homework 4. I think it would be very beneficial for you to review your Homework 4 before starting this one.**

Measuring body fat is not simple. One method requires submerging the body underwater in a tank and measuring the increase in water level. A simpler method for estimating body fat would be preferred. In order to develop such a method, researchers recorded age (years), weight (pounds), height (inches), and three body circumference measurements (around the neck, chest, and abdominal (all in centimeters)) for 252 men. Each mans' percentage of body fat was accurately estimated by an underwater weighing technique (the variable brozek is the percentage of body fat). The hope is to be able to use this data to create a model that will accurately predict body fat percentage, by using just the basic variables recorded, without having to use the tank submerging method. 

The data can be found in the BodyFat data set on Canvas. Download BodyFat.txt, and put it in the same folder as this R Markdown file.

#### 0. Replace the text "< PUT YOUR NAME HERE >" (above next to "author:") with your full name.

#### 0b. Make sure to set your seed since some of the functions randomly split your data (use `set.seed` in the setup code chunk above)!

#### 1. Read in the data set, and call the data frame "bodyfat_orig". Print a summary of the data and make sure the data makes sense. **Remove the "row" column (which contains row numbers) from the data set.** Make sute the class of "bodyfat_orig" is a *data.frame only*.

```{r}
bodyfat_orig <- data.frame(read_table("BodyFat.txt") %>% select(-c('row')))
head(bodyfat_orig)
```

#### 2. Refer back to your Homework 4. In that assignment, you fit this multiple linear regression model: for each of the multiple linear regression assumptions listed below, state if they were met or not met.

1. The Xâ€™s vs Y are linear: Met
2. The residuals are normally distributed and centered at zero: Met
3. The residuals are homoscedastic: Met
4. The model describes all observations (i.e., there are no influential points): Not met
5. No multicollinearity: Not met

#### 3. There is one clear influential point in the data set. Create a new variable called "bodyfat" that contains the bodyfat_orig data set with the influential point removed. Use the bodyfat data set (not the bodyfat_orig data set) throughout the rest of the assignment.

```{r, fig.align='center'}
# Looking at HW4, influential point is observation number 39
bodyfat <- bodyfat_orig[-c(39),]
```

### You should have discovered, from Homework 4, that there is a multicollinearity problem. The goal of this assignment is to continue this analysis by identifying variables to potentially remove from the model to resolve the multicollinearity issues. 

#### 4. Briefly explain why multicollinearity is a problem for multiple linear regression by identifying two consequences of multicollinearity.

Multicollinearity is a problem because it creates problems testing individual regression coefficients due to inflated standard errors. It also can cause coefficients to switch signs.

#### 5. Briefly explain the similarities and differences between the following methods: best subset, forward, backward, and sequential replacement. Do not just copy the algorithm from the class notes - use your own words to explain what these methods are doing.

Forward and Backwards selection both modify the predictor variables. The difference between the two is that Forward adds the variables then evaluates the model, but Backward removes a variables then evaluates.

Best subset tests all possible models and finds the best one, rather than evaluating predictor variables.

Sequential replacement is similar to both Forward and Backward selection. It basically does the same process.

#### 6. Briefly explain how shrinkage methods work (variance-bias tradeoff).

1. Remove some variables from the model
2. Apply a shrinkage method
3. Combine the correlated variables somehow

#### 7. Briefly explain the similarities/difference between ridge regression, LASSO, and elastic net.

Lasso and elastic net are both variable selection methods since they can result in variables being dropped. Ridge regression keeps all variables in model.

Ridge regression doesn't allow coefficients to reach exactly zero, where Elastic net allows coefficients to be zero, along with Lasso.

#### 8. Remember, when coding these methods, the response variable must be the last column in the data set for the `bestglm` function to work. Switch the order of the columns in the data set so that brozek is last.

```{r, fig.align='center'}
# your code here
bodyfat <- bodyfat %>% relocate(brozek, .after = abdom)
```

#### 9. Apply the best subsets variable selection procedure to this data set. You may choose which metric you would like to use (ex: AIC, BIC, PMSE). Output a summary of the "best" model.

```{r, fig.align='center'}
best_subsets_bic <- bestglm(bodyfat,
                            IC = "BIC",
                            method = "exhaustive")

# view variables included in the top 10 models
best_subsets_bic$BestModels

# view a summary of the "best" model
summary(best_subsets_bic$BestModel)
```

#### 10. Apply the forward selection procedure to this data set. You may choose which metric you would like to use (ex: AIC, BIC, PMSE). Output a summary of the "best" model.

```{r, fig.align='center'}
best_subsets_aic <- bestglm(bodyfat,
                            IC = "AIC")
# view variables included in the top 10 models
best_subsets_aic$BestModels

# view a summary of the "best" model
summary(best_subsets_aic$BestModel)
```

#### 11. Apply the backward selection procedure to this data set. You may choose which metric you would like to use (ex: AIC, BIC, PMSE). Output a summary of the "best" model.

```{r, fig.align='center'}
best_subsets_aic <- bestglm(bodyfat,
                            IC = "CV")
# view variables included in the top 10 models
best_subsets_aic$BestModels

# view a summary of the "best" model
summary(best_subsets_aic$BestModel)
```

#### 12. Apply the sequential replacement selection procedure to this data set. You may choose which metric you would like to use (ex: AIC, BIC, PMSE). Output a summary of the "best" model.

```{r, fig.align='center'}
best_subsets_pmse <- bestglm(bodyfat,
                            IC = "CV",
                            method = "seqrep")

# view variables included in the top 10 models
best_subsets_pmse

# view a summary of the "best" model
summary(best_subsets_pmse)
```

#### 13. Apply LASSO to this data set using the MSE metric. Output the coefficient values corresponding to the 1 standard error rule (do not output any plots).

```{r, fig.align='center'}
bodyfat_x <- as.matrix(bodyfat[, 1:6])
bodyfat_y <- bodyfat[, 7]

# use cross validation to pick the "best" (based on MSE) lambda
bodyfat_ridge_cv <- cv.glmnet(x = bodyfat_x,
                          y = bodyfat_y, 
                          type.measure = "mse", 
                          alpha = 1)  # 0 is code for "ridge regression", 1 is code for "LASSO"

# plot (log) lambda vs MSE
autoplot(bodyfat_ridge_cv, label = FALSE) +
  theme_bw() +
  theme(aspect.ratio = 1)

# lambda.min: value of lambda that gives minimum mean cross-validated error
bodyfat_ridge_cv$lambda.min
# lambda.1se: value of lambda within 1 standard error of the minimum 
# cross-validated error
bodyfat_ridge_cv$lambda.1se

coef(bodyfat_ridge_cv, s = "lambda.min")
coef(bodyfat_ridge_cv, s = "lambda.1se")
```

#### 14. Apply Elastic Net to this data set using the MSE metric. Output the coefficient values corresponding to the 1 standard error rule (do not output any plots).

```{r, fig.align='center'}
bodyfat_x <- as.matrix(bodyfat[, 1:6])
bodyfat_y <- bodyfat[, 7]

# use cross validation to pick the "best" (based on MSE) lambda
bodyfat_ridge_cv <- cv.glmnet(x = bodyfat_x,
                          y = bodyfat_y, 
                          type.measure = "mse", 
                          alpha = 1)  # 0.5 is code for "Elastic Net"

# plot (log) lambda vs MSE
autoplot(bodyfat_ridge_cv, label = FALSE) +
  theme_bw() +
  theme(aspect.ratio = 1)

# lambda.min: value of lambda that gives minimum mean cross-validated error
bodyfat_ridge_cv$lambda.min
# lambda.1se: value of lambda within 1 standard error of the minimum 
# cross-validated error
bodyfat_ridge_cv$lambda.1se

coef(bodyfat_ridge_cv, s = "lambda.min")
coef(bodyfat_ridge_cv, s = "lambda.1se")
```  


#### 15. Fill in the table below with "X"s (like the one at the end of the course notes: a row for each variable, a column for each variable selection method, an "X" in a cell means the variable was included for that variable selection method).

Variable     | Best Subset | Forward | Backward | Sequential Replacement | LASSO  | Elastic Net
------------ | ----------- | ------- | -------- | ---------------------- | ------ | -----------
age          |             |         |          |                        |    X   |      X
weight       |       X     |         |    X     |           X            |        |
height       |             |    X    |          |                        |    X   |      X
neck         |             |    X    |          |                        |        |      X
chest        |             |    X    |          |                        |        |
abdom        |       X     |    X    |    X     |           X            |    X   |      X






#### 16. Now that you have seen the various results from the different methods, pick a subset of variables that you will include in the model. Which variables do you choose to include in the model? Why?

I'll pick abdom and weight for my model. Abdom was selected by all methods so it's pretty influential. I also like weight because the best subset selected it.

#### 17. Create the multiple linear regression model with the variables you listed in the previous question (alternatively, you can call the best model using $BestModel). Print a summary of the results. Save the residuals from this model to the bodyfat dataframe.

```{r, fig.align='center'}
# your code here
#From Best Subsets
summary(best_subsets_bic$BestModel)
bodyfat_lm <- best_subsets_bic$BestModel

bodyfat$residuals <- bodyfat_lm$residuals

```



### Now that you have chosen a model, the next several questions ask you to check some of the model assumptions. For each assumption, (1) perform appropriate diagnostics to determine if the assumption is violated, and (2) explain whether or not you think the assumption is violated and why you think that. **Note: you can copy (then modify) a lot of your code from Homework 4 to answer these questions.**



#### 18. (L) The Xs vs Y are linear (use the scatterplot matrix and the partial regression plots)

```{r, fig.align='center'}
pairs(bodyfat)
avPlots(bodyfat_lm)
```

Linearity is met. The graphs show that the model has a linear pattern.

#### 19. (N) The residuals are normally distributed and centered at zero (use the boxplot and normal probability plot)

```{r, fig.align='center'}
autoplot(bodyfat_lm, which = 2, ncol = 1, nrow = 1)

ggplot(data = bodyfat, mapping = aes(y = residuals)) +
  geom_boxplot()
```

This assumption is met. Residuals are normally distributed and are centered at zero.

#### 20. (E) The residuals have equal/constant variance across all values of X (use the residuals vs. fitted values plot)

```{r}
# your code here
(bodyfat_resid_fitted_plot <- autoplot(bodyfat_lm, which = 1, ncol = 1, nrow = 1))
```

This assumption is met. The right sided isn't pulled down by that observation 39 so the right side is more centered.

#### 21. (A) The model describes all observations (i.e., there are no influential points) (use the DFBETAS and DFFITS)

```{r, fig.align='center'}
#DFBETAS
# Weight
bodyfat$dfbetas_weight <- as.vector(dfbetas(bodyfat_lm)[, "weight"])
# plot the DFBETAS against the observation number
dfbetas_plot_weight <- ggplot(data = bodyfat) + 
  geom_point(mapping = aes(x = as.numeric(rownames(bodyfat)), 
                           y = abs(dfbetas_weight))) +
  ylab("Abs(DFBETAS) for weight") +
  xlab("Observation Number") +
  # for n > 30
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(dfbetas_weight))),
             color = "red", 
             linetype = "dashed") + 
  # for n <= 30
  geom_hline(mapping = aes(yintercept = 1),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

# Abdom
bodyfat$dfbetas_abdom <- as.vector(dfbetas(bodyfat_lm)[, "abdom"])
# plot the DFBETAS against the observation number
dfbetas_plot_abdom <- ggplot(data = bodyfat) + 
  geom_point(mapping = aes(x = as.numeric(rownames(bodyfat)), 
                           y = abs(dfbetas_abdom))) +
  ylab("Abs(DFBETAS) for abdom") +
  xlab("Observation Number") +
  # for n > 30
  geom_hline(mapping = aes(yintercept = 2 / sqrt(length(dfbetas_abdom))),
             color = "red", 
             linetype = "dashed") + 
  # for n <= 30
  geom_hline(mapping = aes(yintercept = 1),
             color = "red", 
             linetype = "dashed") +
  theme(aspect.ratio = 1)

#Plot all DFBETAS
grid.arrange(dfbetas_plot_weight,
             dfbetas_plot_abdom,
             nrow = 1)

bodyfat$dffits <- dffits(bodyfat_lm)

# plot the DFFITS against the observation number
ggplot(data = bodyfat) + 
  geom_point(mapping = aes(x = as.numeric(rownames(bodyfat)), 
                           y = abs(dffits))) +
  ylab("Absolute Value of DFFITS for brozek") +
  xlab("Observation Number") +
  # for n > 30
  geom_hline(mapping = aes(yintercept = 2 * sqrt(length(bodyfat_lm$coefficients) /
                                                   length(dffits))),
             color = "red", 
             linetype = "dashed")+
  theme(aspect.ratio = 1)
```
The assumption now fits. There are no influential points really affecting the model.

#### 22. No multicollinearity (use the scatterplot matrix, correlation matrix, and variance inflation factors)

```{r, fig.align='center'}
vif(bodyfat_lm)

corrplot(cor(bodyfat %>% select(c("weight", "abdom", "brozek"))), type = "upper")

pairs(bodyfat %>% select(c("weight", "abdom", "brozek")))

```

There are no multicollinearity in the model. This assumption is now met.

#### 23. Given the results from your model assumption checking, what would you do next to continue this analysis?

Now that the model meets all the assumptions, I would analyze the model and make inferences that are valuable.

#### 24. Briefly summarize what you learned, personally, from this analysis about the statistics, model fitting process, etc.

I really like this process of selecting variables. It's hard to tell which ones are statically significant just by looking at them, then you add in multicollinearity and it adds a whole other element to decipher. R makes it really easy to decide and it's more simple than I thought it would be.

#### 25. Briefly summarize what you learned from this analysis *to a non-statistician*. Write a few sentences about (1) the purpose of this data set and analysis and (2) what you learned about this data set from your analysis. Write your response as if you were addressing a business manager (avoid using statistics jargon) and just provide the main take-aways.

This bodyfat data set and analysis is to find which variables have some sort of significant influence on body fat percentage (borzek).
I learned that surprisingly that as weight increases, body fat percentage decreases (at a really low rate). Also, as abdominal circumference increases, body fat percentage increases.
